[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Uday's AI Blog",
    "section": "",
    "text": "Post With Code\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\nScreening and filtering data from the IGN catalog down to the relevant events for La Palma\n\n\n\nHarlow Malloc\n\n\nSep 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffective Training and Debugging of a Neural Networks\n\n\n\nDebugging NN\n\n\nNeural Network\n\n\ntraining\n\n\n\nProper ways to training and debugging of a neural network\n\n\n\nUday\n\n\nFeb 3, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/data-screening.html",
    "href": "notebooks/data-screening.html",
    "title": "Post With Code",
    "section": "",
    "text": "Load and review the data, check that dates are loaded properly and filter data down to the events in La Palma only. The dataset itself contains all recent IGN earthquake data for Spanish territory.\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('../data/catalogoComunSV_1663233588717.csv', sep=';')\ndf.columns = ['Event', 'Date', 'Time', 'Latitude', 'Longitude', 'Depth(km)', 'Intensity','Magnitude', 'Type Mag', 'Location']\ndf['Time'] = df['Time'].str.strip()\ndf['Date'] = df['Date'].str.strip()\ndf['DateTime'] = (df['Date'] + ' ' + df['Time']).apply(pd.to_datetime, format='%d/%m/%Y %H:%M:%S')\ndf['Timestamp'] = pd.to_numeric(pd.to_datetime(df['DateTime']))\ndf['Date'] = pd.to_datetime(df['Date'], format=\"%d/%m/%Y\")\ndf = df.sort_values(by=['DateTime'], ascending=True)\ndf.head()\n\n\n\n\n\n\n\n\nEvent\nDate\nTime\nLatitude\nLongitude\nDepth(km)\nIntensity\nMagnitude\nType Mag\nLocation\nDateTime\nTimestamp\n\n\n\n\n0\nes2017aaaga\n2017-01-01\n00:48:55\n36.6765\n-11.2736\n0.0\n\n2.8\n6\nSW CABO DE SAN VICENTE\n2017-01-01 00:48:55\n1483231735000000000\n\n\n1\nes2017aaakn\n2017-01-01\n01:28:17\n28.1119\n-16.2225\n21.0\n\n0.9\n4\nATLÁNTICO-CANARIAS\n2017-01-01 01:28:17\n1483234097000000000\n\n\n2\nes2017aaang\n2017-01-01\n01:49:08\n42.0648\n-7.8471\n22.0\n\n2.0\n4\nSW RAIRIZ DE VEIGA.OU\n2017-01-01 01:49:08\n1483235348000000000\n\n\n3\nes2017aabdd\n2017-01-01\n02:36:20\n38.3826\n-9.3767\n11.0\n\n2.2\n4\nATLÁNTICO-PORTUGAL\n2017-01-01 02:36:20\n1483238180000000000\n\n\n4\nes2017aabkh\n2017-01-01\n03:35:03\n36.2492\n-7.8227\n13.0\n\n1.9\n4\nGOLFO DE CÁDIZ\n2017-01-01 03:35:03\n1483241703000000000\ndf.describe()\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nDepth(km)\nMagnitude\nType Mag\nTimestamp\n\n\n\n\ncount\n58036.000000\n58036.000000\n58036.000000\n58036.000000\n58036.000000\n5.803600e+04\n\n\nmean\n34.806854\n-8.356855\n11.450831\n1.881846\n4.031153\n1.596182e+18\n\n\nstd\n5.263327\n7.119341\n11.111965\n0.763577\n0.291332\n4.954865e+16\n\n\nmin\n26.349300\n-19.930400\n0.000000\n-0.500000\n3.000000\n1.483232e+18\n\n\n25%\n28.563800\n-16.672325\n3.000000\n1.400000\n4.000000\n1.559327e+18\n\n\n50%\n35.989200\n-4.785250\n10.000000\n1.800000\n4.000000\n1.612319e+18\n\n\n75%\n37.890700\n-3.234500\n14.300000\n2.400000\n4.000000\n1.635568e+18\n\n\nmax\n44.985000\n5.996600\n115.000000\n6.000000\n6.000000\n1.663232e+18\ndf.plot.scatter(x=\"Longitude\", y=\"Latitude\", figsize=(12,12), grid=\"on\");"
  },
  {
    "objectID": "notebooks/data-screening.html#spatial-plot",
    "href": "notebooks/data-screening.html#spatial-plot",
    "title": "Post With Code",
    "section": "Spatial Plot",
    "text": "Spatial Plot\nScatter plot the spatial locations of events\n\nfrom matplotlib import colormaps\ncmap = colormaps['viridis_r']\nax = df.plot.scatter(x=\"Longitude\", y=\"Latitude\", \n                     s=40-df[\"Depth(km)\"], c=df[\"Magnitude\"], \n                     figsize=(12,10), grid=\"on\", cmap=cmap)\ncolorbar = ax.collections[0].colorbar\ncolorbar.set_label(\"Magnitude\")\n\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Locations of earthquakes on La Palma since 2017.\n\n\n\n\n\n\nTimeline Plot\nScatter plot the event time series and look for any quantization issues. Have times & dates been loaded correctly?\n\nax = df.plot.scatter(x='DateTime', y='Depth(km)', figsize=(20,8))\nax.set_ylim(50,0);\n\n\n\n\n\n\n\n\n\ndf['Swarm'] = 0\ndf.loc[(df['Date'] &gt;= '2017-10-01') & (df['Date'] &lt;= '2017-10-31'), 'Swarm'] = 1\ndf.loc[(df['Date'] &gt;= '2019-02-01') & (df['Date'] &lt;= '2019-02-28'), 'Swarm'] = 2\ndf.loc[(df['Date'] &gt;= '2020-07-21') & (df['Date'] &lt;= '2020-08-05'), 'Swarm'] = 3\ndf.loc[(df['Date'] &gt;= '2020-10-04') & (df['Date'] &lt;= '2020-10-10'), 'Swarm'] = 4\ndf.loc[(df['Date'] &gt;= '2020-10-11') & (df['Date'] &lt;= '2020-10-22'), 'Swarm'] = 5\ndf.loc[(df['Date'] &gt;= '2020-11-15') & (df['Date'] &lt;= '2020-11-29'), 'Swarm'] = 6\ndf.loc[(df['Date'] &gt;= '2020-12-10') & (df['Date'] &lt;= '2020-12-29'), 'Swarm'] = 7\ndf.loc[(df['Date'] &gt;= '2020-12-10') & (df['Date'] &lt;= '2020-12-31'), 'Swarm'] = 8\ndf.loc[(df['Date'] &gt;= '2021-01-15') & (df['Date'] &lt;= '2021-02-07'), 'Swarm'] = 9\ndf.loc[(df['Date'] &gt;= '2021-06-01') & (df['Date'] &lt;= '2021-06-30'), 'Swarm'] = 10\ndf.loc[(df['Date'] &gt;= '2021-07-01'), 'Swarm'] = None\n\n\ndf['Phase'] = 0;\ndf.loc[(df['Date'] &gt;= '2021-09-11') & (df['Date'] &lt;= '2021-09-30'), 'Phase'] = 1\ndf.loc[(df['Date'] &gt;= '2021-10-01') & (df['Date'] &lt;= '2021-11-30'), 'Phase'] = 2\ndf.loc[(df['Date'] &gt;= '2021-12-01') & (df['Date'] &lt;= '2021-12-31'), 'Phase'] = 3\ndf.loc[(df['Date'] &gt;= '2021-12-31'), 'Phase'] = 4\n\n\ndf.to_csv(\"../data/lapalma_ign.csv\", index=False)"
  },
  {
    "objectID": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html",
    "href": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html",
    "title": "Effective Training and Debugging of a Neural Networks",
    "section": "",
    "text": "In this blog, I want to discuss Training and Debugging of a NN in a practical manner.\nI am taking a cyber troll dataset. It is a classification data with labels aggressive or not. This is mostly inspired from this blog.\nWhenever I train any neural network, I will divide that into subtasks as below. I am assuming, you already set your project goals and evaluation metrics.\n##basic imports\nimport numpy as np\nimport pandas as pd\nimport random as rn\nimport tensorflow as tf\nfrom tensorflow.keras.layers import LSTM, GRU, Dense, Input, Embedding\nfrom tensorflow.keras.models import Model"
  },
  {
    "objectID": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#data-processing",
    "href": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#data-processing",
    "title": "Effective Training and Debugging of a Neural Networks",
    "section": "Data Processing",
    "text": "Data Processing\n\n##reading the data\ncyber_troll_data = pd.read_json('Dataset for Detection of Cyber-Trolls.json', lines=True)\ncyber_troll_data.head(2)\n\n\n\n\n\n\n\n\ncontent\nannotation\nextras\nmetadata\n\n\n\n\n0\nGet fucking real dude.\n{'notes': '', 'label': ['1']}\nNaN\n{'first_done_at': 1527503426000, 'last_updated...\n\n\n1\nShe is as dirty as they come and that crook R...\n{'notes': '', 'label': ['1']}\nNaN\n{'first_done_at': 1527503426000, 'last_updated...\n\n\n\n\n\n\n\n\n#basic preprocessing\ncyber_troll_data['label']=cyber_troll_data.annotation.apply(lambda x: int(x['label'][0]))\ncyber_troll_data = cyber_troll_data[['content', 'label']]\ncyber_troll_data.head()\n\n\n\n\n\n\n\n\ncontent\nlabel\n\n\n\n\n0\nGet fucking real dude.\n1\n\n\n1\nShe is as dirty as they come and that crook R...\n1\n\n\n2\nwhy did you fuck it up. I could do it all day ...\n1\n\n\n3\nDude they dont finish enclosing the fucking sh...\n1\n\n\n4\nWTF are you talking about Men? No men thats no...\n1\n\n\n\n\n\n\n\n\n#its a imbalance one\ncyber_troll_data.label.value_counts()\n\n0    12179\n1     7822\nName: label, dtype: int64\n\n\n\n##splitting data into train, validation and Test data. \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(cyber_troll_data.content, cyber_troll_data.label, \n                                                    test_size=0.40, stratify=cyber_troll_data.label, random_state=54)\n\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, \n                                                    test_size=0.50, stratify=y_test, random_state=32)\n\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(X_train)\n\n\nX_train_tokens = tokenizer.texts_to_sequences(X_train)\nX_test_tokens = tokenizer.texts_to_sequences(X_test)\nX_val_tokens = tokenizer.texts_to_sequences(X_val)\n\n\nnumber_vocab = len(tokenizer.word_index)+1\n\nX_train_pad_tokens = tf.keras.preprocessing.sequence.pad_sequences(X_train_tokens, maxlen=24, padding='post', truncating='post')\nX_test_pad_tokens = tf.keras.preprocessing.sequence.pad_sequences(X_test_tokens, maxlen=24, padding='post', truncating='post')\nX_val_pad_tokens = tf.keras.preprocessing.sequence.pad_sequences(X_val_tokens, maxlen=24, padding='post', truncating='post')\n\nWe prepared the data. I am not doing perfect preprocessing and tokenization. You can do preprocessing in a better way.\nWe have,\nX_train_pad_tokens, y_train  --&gt; To train   X_val_pad_tokens, y_val   --&gt; To validate and Tune   X_test_pad_tokens, y_test  --&gt; Don't use this data while trainig. Only use this after you are done with all the modelling.\nI am creating Training and Validation datasets to iterate over those using the tf.data pipeline. Please use less data as of now because, it will be easier to debug and easier to know about the error if we have any in our network, I will discuss this below. I am only using the first 100 data points with a batch size of 32.\n\n##Creating the dataset( only 100 data points and will explain why after trainig process.) \ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train_pad_tokens[0:100], y_train[0:100]))\ntrain_dataset = train_dataset.shuffle(1000).batch(32, drop_remainder=True)\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n##creating test dataset using tf.data\nval_dataset = tf.data.Dataset.from_tensor_slices((X_val_pad_tokens[0:100], y_val[0:100]))\nval_dataset = val_dataset.batch(32, drop_remainder=True)\nval_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n\nChecking the data pairing issue and check the data given to neural network is correct or not. If it got corrupted, check/debug the data pipleline and rectify it. If you have images, try to plot the images and check.\nbelow, i have written a basic for loop to print. You can also print the words corresponding to the numbers and check.\n\nfor input_text, output_label in train_dataset:\n    print(input_text[0:3], output_label[0:3])\n    break\n\ntf.Tensor(\n[[ 186   89  741    5  385   43   11  127  919 1082  157    1    9  251\n     5  628    3 6970    5   11 4641   30    6   40]\n [  27    3   26   28 1021   29    6    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0]\n [4647   72  606   43   16  684  223    1    9    3 4648  923    0    0\n     0    0    0    0    0    0    0    0    0    0]], shape=(3, 24), dtype=int32) tf.Tensor([0 1 1], shape=(3,), dtype=int32)"
  },
  {
    "objectID": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#creating-a-neural-network",
    "href": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#creating-a-neural-network",
    "title": "Effective Training and Debugging of a Neural Networks",
    "section": "Creating a Neural Network",
    "text": "Creating a Neural Network\nSome of the rules to follow while writing/training your Neural Network.\n\nStart with a simple architecture - We are doing a text classification so, we can try a single layer LSTM.\nUse well studied default parameters like activation = relu, optimizer = adam, initialization = he for relu and Glorot for sigmoid/tanh. To know more about this, please read this blog.\nFix the random seeds so that we can reproduce the initializations/results to tune our models. - You have to fix all the random seeds in your model.\nNormalize the input data.\n\nI am writing a simple LSTM model by following all the above rules.\n\n##LSTM\n\n##fixing numpy RS\nnp.random.seed(42)\n\n##fixing tensorflow RS\ntf.random.set_seed(32)\n\n##python RS\nrn.seed(12)\n\n\n##model\ndef get_model():\n    input_layer = Input(shape=(24,), name=\"input_layer\")\n    ##i am initilizing randomly. But you can use predefined embeddings. \n    x_embedd = Embedding(input_dim=number_vocab, output_dim=100, input_length=24, mask_zero=True, \n                        embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23),\n                         name=\"Embedding_layer\")(input_layer)\n    \n    x_lstm = LSTM(units=20, activation='tanh', recurrent_activation='sigmoid', use_bias=True, \n                 kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n                 recurrent_initializer=tf.keras.initializers.orthogonal(seed=54),\n                 bias_initializer=tf.keras.initializers.zeros(), name=\"LSTM_layer\")(x_embedd)\n    \n    x_out = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45),\n                  name=\"output_layer\")(x_lstm)\n    \n    basic_lstm_model = Model(inputs=input_layer, outputs=x_out, name=\"basic_lstm_model\")\n    \n    return basic_lstm_model\n\n\nbasic_lstm_model = get_model()\nbasic_lstm_model_anothertest = get_model()\n\n Now i created two models named basic_lstm_model, basic_lstm_model_anothertest. Those two model initial weights will be the same because of the fixed random seed. This removes a factor of a variation and very useful to tune parameters by doing some experimentation on the same weight initialization.\nwe can check this as below.\n\n[np.all(basic_lstm_model.get_weights()[i]==basic_lstm_model_anothertest.get_weights()[i]) \\\n for i in range(len(basic_lstm_model.get_weights()))]\n\n[True, True, True, True, True, True]"
  },
  {
    "objectID": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#training-a-nn",
    "href": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#training-a-nn",
    "title": "Effective Training and Debugging of a Neural Networks",
    "section": "Training a NN",
    "text": "Training a NN\nLoss functions - If we calculate the loss in the wrong manner, we will get the wrong gradients and it doesn’t learn perfectly.\nSome of the mistakes in Loss functions:\n\none of the main mistakes in the loss creation is giving wrong inputs to the loss function. If we are using the cross-entropy, you have to give one-hot vector as input otherwise, use sparse_categorical_crossentropy(no need to give the one-hot vectors).\nIf you are using a function that calculates the loss using unnormalized logits, don’t give the probability output as input to the loss function. ( check logits parameter in the tensorflow loss functions)\nIt is useful to mask unnecessary output while calculating loss. Eg: don’t include output at the padded word position while calculation loss.\nSelecting a loss function that allowing the calculation of large error values. Because of this, your loss may explode, you may get NaN and it affects the gradients too.\n\n\n##masked loss Eg for sequence output. \ndef maskedLoss(y_true, y_pred):\n    #getting mask value\n    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n    \n    #calculating the loss\n    loss_ = loss_function(y_true, y_pred)\n    \n    #converting mask dtype to loss_ dtype\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    \n    #applying the mask to loss\n    loss_ = loss_*mask\n    \n    #getting mean over all the values\n    loss_ = tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n    return loss_\n\n\n##creating a loss object for this classification problem\nloss_function = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction='auto')\n\nTraining and validation functions\n\nWe have to take care of the toggling training flag because some of the layers behaves differently in training and testing.\n\n\n#optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n#trainign function\n@tf.function\ndef train_step(input_vector, output_vector,loss_fn):\n    #taping the gradients\n    with tf.GradientTape() as tape:\n        #for ward prop\n        output_predicted = basic_lstm_model(inputs=input_vector, training=True)\n        #loss calculation\n        loss = loss_fn(output_vector, output_predicted)\n    #getting gradients\n    gradients = tape.gradient(loss, basic_lstm_model.trainable_variables)\n    #applying gradients\n    optimizer.apply_gradients(zip(gradients, basic_lstm_model.trainable_variables))\n    return loss, output_predicted\n\n#validation function\n@tf.function\ndef val_step(input_vector, output_vector, loss_fn):\n    #forward prop\n    output_predicted = basic_lstm_model(inputs=input_vector, training=False)\n    #loss calculation\n    loss = loss_fn(output_vector, output_predicted)\n    return loss, output_predicted\n\nTraining the NN with proper data.\n\nWhile Training the model, I suggest you don't write the complex pipelining of the data and train your network at the start. If you do this, finding the bugs in your network is very difficult. Just get a few instances of data( maybe 10% of your total train data if you have 10K records) into your RAM and try to train your network. In this case, I have total data in my RAM so, I will slice a few batches and try to train the network.\nI will suggest you don't include the data augmentation as of now. It is useful for regularizing the model but try to avoid it at the start. Even if you do data augmentation, be careful about the labels. Eg: In the segmentation task, if you flip the image, you have to flip the label image as well.\nCheck for casting issues. Eg. If layer needs int8, give the int8 value only as input. If you have float values, just cast the dtype. If data stored in the disk is float32, load the data into RAM with the same dtype.\nCheck the data pairing issue i.e. while giving the train data, you have to give the correct pairs of x and y. Training the NN with proper data.\n\nTraining the NN with data for 2 epochs and printing batchwise loss and finally getting mean of all those. Even if you use the .fit method of Keras API, it prints the aggregated value of loss/metric as part of verbose. You can check that aggregate class here\n\n##training\nEPOCHS=2\n\n##metrics # Even if you use .fit method, it alsocalculates batchwise loss/metric and aggregates those.  \ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\nval_loss = tf.keras.metrics.Mean(name='test_loss')\n\nfor epoch in range(EPOCHS):\n    #losses\n    train_loss.reset_states()\n    val_loss.reset_states()\n    \n    #training\n    print('Batchwise Train loss')\n    for text_seq, label_seq in train_dataset:\n        loss_, pred_out = train_step(text_seq, label_seq, loss_function)\n        print(loss_)\n        train_loss(loss_)\n    \n    #validation\n    print('Batchwise Val loss')\n    for text_seq_val, label_seq_val in val_dataset:\n        loss_test, pred_out_test = val_step(text_seq_val, label_seq_val, loss_function)\n        print(loss_test)\n        val_loss(loss_test)\n    \n    template = 'Epoch {}, Mean Loss: {}, Mean Val Loss: {}'\n    \n    print(template.format(epoch+1, train_loss.result(), val_loss.result()))\n    print('-'*50)\n\nBatchwise Train loss\ntf.Tensor(0.69066906, shape=(), dtype=float32)\ntf.Tensor(0.6978342, shape=(), dtype=float32)\ntf.Tensor(0.7214557, shape=(), dtype=float32)\nBatchwise Val loss\ntf.Tensor(0.7479876, shape=(), dtype=float32)\ntf.Tensor(0.6868224, shape=(), dtype=float32)\ntf.Tensor(0.71952724, shape=(), dtype=float32)\nEpoch 1, Mean Loss: 0.7033197283744812, Mean Val Loss: 0.7181124687194824\n--------------------------------------------------\nBatchwise Train loss\ntf.Tensor(0.6816538, shape=(), dtype=float32)\ntf.Tensor(0.69258916, shape=(), dtype=float32)\ntf.Tensor(0.6689039, shape=(), dtype=float32)\nBatchwise Val loss\ntf.Tensor(0.744266, shape=(), dtype=float32)\ntf.Tensor(0.681653, shape=(), dtype=float32)\ntf.Tensor(0.71762204, shape=(), dtype=float32)\nEpoch 2, Mean Loss: 0.6810489296913147, Mean Val Loss: 0.7145137190818787\n--------------------------------------------------"
  },
  {
    "objectID": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#debugging-and-enhancing-nn",
    "href": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#debugging-and-enhancing-nn",
    "title": "Effective Training and Debugging of a Neural Networks",
    "section": "Debugging and Enhancing NN",
    "text": "Debugging and Enhancing NN\nTill now, we have created a basic NN for our problem and trained the NN. Now I will discuss some hacks to debug and enhance your training process to get better results.\n\nUsing Basic print statements and checking the shapes of input and output of every layer. Using this, we can remove the shape related error or basic errors related to output while creating a model. If you want to print in tensorflow code, please use tf.print\nWith Eager execution, we can debug our code very easily. it can be done using pdb or using any ide. You have to set tf.config.experimental_run_functions_eagerly(True) to debug your tf2.0 functions.\n\n\n##LSTM\n\ntf.config.experimental_run_functions_eagerly(True)\n\n##fixing numpy RS\nnp.random.seed(42)\n\n##fixing tensorflow RS\ntf.random.set_seed(32)\n\n##python RS\nrn.seed(12)\n\nimport pdb\n\n##model\ndef get_model_debug():\n    input_layer_d = Input(shape=(24,), name=\"input_layer\")\n    ##i am initilizing randomly. But you can use predefined embeddings. \n    x_embedd_d= Embedding(input_dim=number_vocab, output_dim=100, input_length=24, mask_zero=True, \n                        embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23),\n                         name=\"Embedding_layer\")(input_layer_d)\n    \n    #LSTM\n    x_lstm_d = LSTM(units=20, activation='tanh', recurrent_activation='sigmoid', use_bias=True, \n                 kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n                 recurrent_initializer=tf.keras.initializers.orthogonal(seed=54),\n                 bias_initializer=tf.keras.initializers.zeros(), name=\"LSTM_layer\")(x_embedd_d)\n    \n    #trace\n    pdb.set_trace()\n    \n    x_out_d = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45),\n                  name=\"output_layer\")(x_lstm_d)\n    \n    basic_lstm_model_d = Model(inputs=input_layer_d, outputs=x_out_d, name=\"basic_lstm_model_d\")\n    \n    return basic_lstm_model_d\n\n\nbasic_model_debug = get_model_debug()\n\ntf.config.experimental_run_functions_eagerly(False)\n\n&gt; &lt;ipython-input-14-476c66b41633&gt;(31)get_model_debug()\n-&gt; x_out_d = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45),\n(Pdb)  locals()\n{'input_layer_d': &lt;tf.Tensor 'input_layer_2:0' shape=(None, 24) dtype=float32&gt;, 'x_embedd_d': &lt;tf.Tensor 'Embedding_layer_2/Identity:0' shape=(None, 24, 100) dtype=float32&gt;, 'x_lstm_d': &lt;tf.Tensor 'LSTM_layer_2/Identity:0' shape=(None, 20) dtype=float32&gt;}\n(Pdb)  n\n&gt; &lt;ipython-input-14-476c66b41633&gt;(32)get_model_debug()\n-&gt; name=\"output_layer\")(x_lstm_d)\n(Pdb)  n\n&gt; &lt;ipython-input-14-476c66b41633&gt;(34)get_model_debug()\n-&gt; basic_lstm_model_d = Model(inputs=input_layer_d, outputs=x_out_d, name=\"basic_lstm_model_d\")\n(Pdb)  c\n\n\nYou can also Debug the Trainig loopas shown below.\nFor PDB instrctions, please check this PDF.\nMy preference and suggestion is to use IDE Debugger\n\n##training\nEPOCHS=1\n##metrics # Even if you use .fit method, it alsocalculates batchwise loss/metric and aggregates those.  \ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\n\ntf.config.experimental_run_functions_eagerly(True)\nfor epoch in range(EPOCHS):\n    train_loss.reset_states()\n    \n    print('Batchwise Train loss')\n    for text_seq, label_seq in train_dataset:\n        pdb.set_trace()\n        loss_, pred_out = train_step(text_seq, label_seq, loss_function)\n        print(loss_)\n        train_loss(loss_)\n    \n    template = 'Epoch {}, Mean Loss: {}'\n    \n    print(template.format(epoch+1, train_loss.result()))\n    print('-'*50)\ntf.config.experimental_run_functions_eagerly(False)\n\nBatchwise Train loss\n&gt; &lt;ipython-input-15-aa3750dbfb83&gt;(13)&lt;module&gt;()\n-&gt; loss_, pred_out = train_step(text_seq, label_seq, loss_function)\n(Pdb)  s\n--Call--\n&gt; d:\\softwares\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py(551)__call__()\n-&gt; def __call__(self, *args, **kwds):\n(Pdb)  c\ntf.Tensor(0.66431165, shape=(), dtype=float32)\n&gt; &lt;ipython-input-15-aa3750dbfb83&gt;(12)&lt;module&gt;()\n-&gt; pdb.set_trace()\n(Pdb)  c\ntf.Tensor(0.6668887, shape=(), dtype=float32)\n&gt; &lt;ipython-input-15-aa3750dbfb83&gt;(13)&lt;module&gt;()\n-&gt; loss_, pred_out = train_step(text_seq, label_seq, loss_function)\n(Pdb)  c\ntf.Tensor(0.6523603, shape=(), dtype=float32)\nEpoch 1, Mean Loss: 0.6611868739128113\n--------------------------------------------------\n\n\n\nOnce you are done with the creation of the model, Try to Train the model with less data( i have taken 100 samples) and try to overfit the model to that data. To do so we increase the capacity of our model (e.g. add layers or filters) and verify that we can reach the lowest achievable loss (e.g. zero). If your model is unable to overfit a few data points, then either it’s too small (which is unlikely in today’s age), or something is wrong in its structure or the learning algorithm. check for bugs and try to remove those. I will discuss some of the bugs below. If this model is working fine without any bugs, you can train with full data.\nTensorboard is another important tool to debug NN while training. You can visualize the Loss, metrics, gradient/output histograms, distributions, graph and many more. I am writing code to plot all these in the tensorboard.\nAs of now, we are printing/plotting the Mean loss/metric for all the batches in one epoch and, based on this we are analyzing the model performance. This may lead to wrong models for some of the loss functions/metrics. Even if you use the smoothing, it is not an accurate one, it will get an exponentially weighted average over batch-wise loss/metric. so Try to get a loss/metric for entire data of train and Val/test. If you have time/space constraint, at least get for the val/test data. Eg: Mean of Cross entropy over batches is equal to the cross-entropy over total data but not for AUC/F1 score.\nBelow I have written code that calculates loss and metric(AUC) over batches and gets the mean as well as a total loss at once and a better Training and validation functions with tensorboard. please look into it.\n\n\n##training\n\n##model creation\nbasic_lstm_model = get_model()\n\n##optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n\n##metric\nfrom sklearn.metrics import roc_auc_score\n\n##train step function to train\n@tf.function\ndef train_step(input_vector, output_vector,loss_fn):\n    with tf.GradientTape() as tape:\n        #forward propagation\n        output_predicted = basic_lstm_model(inputs=input_vector, training=True)\n        #loss\n        loss = loss_fn(output_vector, output_predicted)\n    #getting gradients\n    gradients = tape.gradient(loss, basic_lstm_model.trainable_variables)\n    #applying gradients\n    optimizer.apply_gradients(zip(gradients, basic_lstm_model.trainable_variables))\n    return loss, output_predicted, gradients\n\n##validation step function\n@tf.function\ndef val_step(input_vector, output_vector, loss_fn):\n    #getting output of validation data\n    output_predicted = basic_lstm_model(inputs=input_vector, training=False)\n    #loss calculation\n    loss = loss_fn(output_vector, output_predicted)\n    return loss, output_predicted\n\nimport math\n\n#batch size\nBATCH_SIZE=32\n##number of epochs\nEPOCHS=10\n\n##metrics # Even if you use .fit method, it alsocalculates batchwise loss/metric and aggregates those.  \ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\nval_loss = tf.keras.metrics.Mean(name='val_loss')\ntrain_metric = tf.keras.metrics.Mean(name=\"train_auc\")\nval_metric = tf.keras.metrics.Mean(name=\"val_metric\")\n\n#tensorboard file writers\nwtrain = tf.summary.create_file_writer(logdir='logs\\\\train')\nwval = tf.summary.create_file_writer(logdir='logs\\\\val')\n\n\n#no of data points/batch_size i.e number of iterations in the one epoch\niters = math.ceil(100/BATCH_SIZE) \n\n#training anf validating\nfor epoch in range(EPOCHS):\n    \n    #resetting the states of the loss and metrics\n    train_loss.reset_states()\n    val_loss.reset_states()\n    train_metric.reset_states()\n    val_metric.reset_states()\n    \n    ##counter for train loop iteration\n    counter = 0\n    \n    #lists to save true and validation data. \n    train_true = []\n    train_predicted = []\n    val_true = []\n    val_predicted = []\n    \n    #ietrating over train data batch by batch\n    for text_seq, label_seq in train_dataset:\n        #train step\n        loss_, pred_out, gradients = train_step(text_seq, label_seq, loss_function)\n        #adding loss to train loss\n        train_loss(loss_)\n        #counting the step number\n        temp_step = epoch*iters+counter\n        counter = counter + 1\n        \n        #calculating AUC for batch\n        batch_metric = roc_auc_score(label_seq, pred_out)\n        train_metric(batch_metric)\n        \n        #appending it to list\n        train_predicted.append(pred_out)\n        train_true.append(label_seq)\n        \n        ##tensorboard \n        with tf.name_scope('per_step_training'):\n            with wtrain.as_default():\n                tf.summary.scalar(\"batch_loss\", loss_, step=temp_step)\n                tf.summary.scalar('batch_metric', batch_metric, step=temp_step)\n        with tf.name_scope(\"per_batch_gradients\"):\n            with wtrain.as_default():\n                for i in range(len(basic_lstm_model.trainable_variables)):\n                    name_temp = basic_lstm_model.trainable_variables[i].name\n                    tf.summary.histogram(name_temp, gradients[i], step=temp_step)\n    \n    #calculating the final loss and metric\n    train_true = tf.concat(train_true, axis=0)\n    train_predicted = tf.concat(train_predicted, axis=0)\n    train_loss_final = loss_function(train_true, train_predicted)\n    train_metric_auc = roc_auc_score(train_true, train_predicted)\n    \n    #validation data\n    for text_seq_val, label_seq_val in val_dataset:\n        #getting val output\n        loss_val, pred_out_val = val_step(text_seq_val, label_seq_val, loss_function)\n        #appending to lists\n        val_true.append(label_seq_val)\n        val_predicted.append(pred_out_val)\n        val_loss(loss_val)\n        \n        #calculating metric\n        batch_metric_val = roc_auc_score(label_seq_val, pred_out_val)\n        val_metric(batch_metric_val)\n    \n    \n    #calculating final loss and metric   \n    val_true = tf.concat(val_true, axis=0)\n    val_predicted = tf.concat(val_predicted, axis=0)\n    val_loss_final = loss_function(val_true, val_predicted)\n    val_metric_auc = roc_auc_score(val_true, val_predicted)\n    \n    #printing\n    template = '''Epoch {}, Train Loss: {:0.6f}, Mean batch Train Loss: {:0.6f}, AUC: {:0.5f}, Mean batch Train AUC: {:0.5f},\n    Val Loss: {:0.6f}, Mean batch Val Loss: {:0.6f}, Val AUC: {:0.5f}, Mean batch Val AUC: {:0.5f}'''\n    \n    print(template.format(epoch+1, train_loss_final.numpy(), train_loss.result(), \n                          train_metric_auc, train_metric.result(), val_loss_final.numpy(),\n                          val_loss.result(), val_metric_auc, val_metric.result()))\n    print('-'*30)\n    \n    #tensorboard\n    with tf.name_scope(\"per_epoch_loss_metric\"):\n        with wtrain.as_default():\n            tf.summary.scalar(\"mean_loss\", train_loss.result().numpy(), step=epoch)\n            tf.summary.scalar('loss', train_loss_final.numpy(), step=epoch)\n            tf.summary.scalar('metric', train_metric_auc, step=epoch)\n            tf.summary.scalar('mean_metric', train_metric.result().numpy(), step=epoch)\n        with wval.as_default():\n            tf.summary.scalar('mean_loss', val_loss.result().numpy(), step=epoch)\n            tf.summary.scalar('loss', val_loss_final.numpy(), step=epoch)\n            tf.summary.scalar('metric', val_metric_auc, step=epoch)\n            tf.summary.scalar('mean_metric', val_metric.result().numpy(), step=epoch)\n\nEpoch 1, Train Loss: 0.700775, Mean batch Train Loss: 0.700775, AUC: 0.46829, Mean batch Train AUC: 0.45378,\n    Val Loss: 0.704532, Mean batch Val Loss: 0.704532, Val AUC: 0.48223, Mean batch Val AUC: 0.48844\n------------------------------\nEpoch 2, Train Loss: 0.596350, Mean batch Train Loss: 0.596350, AUC: 0.86608, Mean batch Train AUC: 0.86355,\n    Val Loss: 0.691127, Mean batch Val Loss: 0.691127, Val AUC: 0.52128, Mean batch Val AUC: 0.53295\n------------------------------\nEpoch 3, Train Loss: 0.508518, Mean batch Train Loss: 0.508518, AUC: 0.98973, Mean batch Train AUC: 0.98923,\n    Val Loss: 0.681388, Mean batch Val Loss: 0.681388, Val AUC: 0.55682, Mean batch Val AUC: 0.57112\n------------------------------\nEpoch 4, Train Loss: 0.441114, Mean batch Train Loss: 0.441114, AUC: 0.99554, Mean batch Train AUC: 0.99460,\n    Val Loss: 0.673574, Mean batch Val Loss: 0.673574, Val AUC: 0.58578, Mean batch Val AUC: 0.60539\n------------------------------\nEpoch 5, Train Loss: 0.368985, Mean batch Train Loss: 0.368985, AUC: 0.99868, Mean batch Train AUC: 0.99861,\n    Val Loss: 0.667929, Mean batch Val Loss: 0.667929, Val AUC: 0.61167, Mean batch Val AUC: 0.62760\n------------------------------\nEpoch 6, Train Loss: 0.306646, Mean batch Train Loss: 0.306646, AUC: 0.99956, Mean batch Train AUC: 1.00000,\n    Val Loss: 0.664882, Mean batch Val Loss: 0.664882, Val AUC: 0.62835, Mean batch Val AUC: 0.63807\n------------------------------\nEpoch 7, Train Loss: 0.249700, Mean batch Train Loss: 0.249700, AUC: 1.00000, Mean batch Train AUC: 1.00000,\n    Val Loss: 0.666024, Mean batch Val Loss: 0.666024, Val AUC: 0.63756, Mean batch Val AUC: 0.64217\n------------------------------\nEpoch 8, Train Loss: 0.195906, Mean batch Train Loss: 0.195906, AUC: 1.00000, Mean batch Train AUC: 1.00000,\n    Val Loss: 0.671024, Mean batch Val Loss: 0.671024, Val AUC: 0.64063, Mean batch Val AUC: 0.64618\n------------------------------\nEpoch 9, Train Loss: 0.151549, Mean batch Train Loss: 0.151549, AUC: 1.00000, Mean batch Train AUC: 1.00000,\n    Val Loss: 0.679804, Mean batch Val Loss: 0.679804, Val AUC: 0.64458, Mean batch Val AUC: 0.64464\n------------------------------\nEpoch 10, Train Loss: 0.111988, Mean batch Train Loss: 0.111988, AUC: 1.00000, Mean batch Train AUC: 1.00000,\n    Val Loss: 0.695000, Mean batch Val Loss: 0.695000, Val AUC: 0.64283, Mean batch Val AUC: 0.64751\n------------------------------\n\n\nI trained the model for 10 epochs and my loss is decreasing and AUC of train data became 1(overfit). But some times it may not overfit to the model. If it is not overfitting, there may be so many reasons like code written to create the model is incorrect, the model is not capable of learning the data, learning problems like vanishing or exploding gradients and many more. I will discuss these problems below and These problems may occur even while training with total data.\n\nCheck whether forward propagation is correct or not\nwhile training NN, we will use the vectorizing implementations of data manipulation. If we did any mistake in these implementations, our training process will give bad results. We can verify this with a simple hack using back prop dependency. Below are the steps to do.\n\nTake a few data points. Here I am taking 5 data points. You can get it from the data or you can generate random data with the same shape.\ndo forward propagation on the model we created with the above batch data.\nwrite a loss function that takes the true values, predicted values and returns loss as sum of the i^th data point output where i less than 5. I am using 3.\ndo the back prop and check the gradients with respect to the input data points. If you are getting non zero gradients only for i-th data point, your forward propagation is right otherwise, there is some error in the forward propagation and you have to debug the code to check the error.\n\nIn the implementation below, I have written basic implementation, not included any tensorboard/metrics and there is no need for those as well.\n\nNote: Gradient won’t flow through the embedding layer so you will get None gradients if you calculate the gradient with of loss with respect to the input. If you have the embedding layer at starting, please remove the embedding layer and give the input directly to the next layer. It is very easy to do because This layer can only be used as the first layer in a model.\n\n\n##same model with name changes and without emedding layer.\ndef get_model_check():\n    ##directly using embedding dimention of 1. It is only for checking so no problem with it. \n    input_layer = Input(shape=(24, 1), batch_size=10, name=\"input_layer_debug\")\n    \n    ##i am initilizing randomly. But you can use predefined embeddings. \n    #x_embedd = Embedding(input_dim=13732, output_dim=100, input_length=24, mask_zero=True, \n                        #embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23),\n                         #name=\"Embedding_layer\")(input_layer)\n    \n    x_lstm = LSTM(units=20, activation='tanh', recurrent_activation='sigmoid', use_bias=True, \n                 kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n                 recurrent_initializer=tf.keras.initializers.orthogonal(seed=54),\n                 bias_initializer=tf.keras.initializers.zeros(), name=\"LSTM_layer_debug\")(input_layer)\n    \n    x_out = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45),\n                  name=\"output_layer_debug\")(x_lstm)\n    \n    basic_model_debug = Model(inputs=input_layer, outputs=x_out, name=\"basic_lstm_model_debug\")\n    \n    return basic_model_debug\n\nbasic_model_debug = get_model_check()\n\n##generated random 5 data points of shape (24,1) i.e 4 time steps and 1 dim embedding. \ntemp_features = np.random.randint(low=1, high=5, size=(5,24, 1))\n\n##generated the a random output zero or 1. I think, there is no use for this as well because \n#we will calculate loss only with predicted values\ntemp_outs = np.random.randint(0, 2, size=(5,1))\n\ndef loss_to_ckgrads(y_true, y_pred):\n    #y_pred is one dimentional you can give directly one data point prediction as loss. \n    #I am giving loss as 3rd data point prediction so we will get non zero gradients only for 3rd data point. \n    #if your prediction is sequence, please add all the i-th data point predictions and return those. \n    return y_pred[2]\n\ndef get_gradient(model, x_tensor):\n    #taping the gradients\n    with tf.GradientTape() as tape:\n        #explicitly telling to watch for input vector. it won't watch with repect to any inputs by default.\n        #it only watches the gradents with weight vectors\n        tape.watch(x_tensor)\n        #model predictions\n        preds = model(x_tensor)\n        #getting the loss\n        loss = loss_to_ckgrads(temp_outs, preds)\n    #getting the gradients    \n    grads = tape.gradient(loss, x_tensor)\n    return grads\n##making temp_feature as varible. We can get the gradients only if it is a varible so chnaging it to variable\ntemp_features = tf.Variable(tf.convert_to_tensor(temp_features, dtype=tf.float32))\n##\ngrads = get_gradient(basic_model_debug, temp_features)\nfor i in grads:\n    #checking whether all zeros or not\n    #except 3rd all the grdients should be zero i.e True\n    print(all(i==0))\n\nTrue\nTrue\nFalse\nTrue\nTrue\n\n\n If you are not getting all true except the i-th one, you may have any issue in your code. You have to check that and resolve it. Without that, don’t go to another step.\n\n\nWhat to do when Loss Explodes\nwhile training NN, you may get NaN/inf loss becuase of large or small values. Below are some causes\n\nNumerical stability issues.\n\nCheck the multiplications, if you are multiplying so many tensors at once, apply log and make it to addition.\nCheck for the division operation. any zero division is happening or not. Try to add a small constant like 1e-12 to the denominator.\nCheck the softmax function. If your vocab size if very large, try not to use the softmax function. calculate the loss based on the logits.\n\nIf the updates to the weights are very large, you may get numerical instability and it may explode.\n\nCheck for the Learning rate. If the learning rate is high, you may get this problem as well.\nCheck for the exploding gradient problem. In tensorboard, you can visualize the gradient histograms and you can check the problem. If gradients are exploding, try to clip the gradients. You can apply tf.linalg.normalize or tf.clip_by_value to your gradients after getting gradients from the GradientTape.\n\nIt may occur because of a poor choice of loss function i.e. allowing the calculation of large error values.\nIt may occur because of the poor data preparation i.e. allowing large differences in the target variables.\n\n\n\nWhat to do when loss Increases\nwhile training NN, our loss may increase some times. Below are some causes\n\nCheck for the Learning rate. If the learning rate is high, you may get this problem as well.\nCheck for the wrong loss function. Especially sign of the loss function.\nActivation functions applying over wrong dimensions. (you can find this out using the point number 1(checking forward propagation is correct or not)\n\n\n\nWhat to do when loss Oscillate\nwhile training NN, our loss may oscillate. Below are some causes\n\nCheck for the Learning rate. If the learning rate is high, you may get this problem as well.\nSometimes it may occur because of the exploding gradient problem. so check for that one as well. You can check this using the Tensorboard.\nIt may occur due to data pairing issues/data corruption. We already discussed this. so make sure to get the proper data.\n\n\n\nWhat to do when loss is constant\nwhile training NN, our loss constant. Below are some causes\n\nIf the updates to the weights are very low, you may end up in the same position.\n\nCheck for the learning rate. If the learning rate is low, our weights won’t update much so you may get this problem.\nCheck for Vanishing Gradient problem. In Tensorboard, you can visualize the gradient histograms and you can check the problem.\n\nYou can solve this by changing the activations to relu/leaky relu.\nYou can add skip connections to an easier flow of gradients.\nIf you have long sequences in RNN, you can divide into smaller ones and train with stateful LSTM’s(Truncated Back prop)\nBetter weight initialization may reduce this.\n\n\nToo much regularization may also cause this.\nIf you are using Relu activation, it may occur due to the dead neurons.\nIncorrect inputs to the loss function. I already discussed this while discussing the loss functions.\n\n\n\nWhat if we get memory Errors\nwhile training NN, many people face the memory exhaust errors because of the computing constraints.\n\nIf you are getting GPU memory exhaust error, try to reduce the batch size and train the neural network.\nIf your data doesn’t fit into the RAM you have, Try to create a data pipeline using tf.data or Keras/Python Data Generators and load the batchwise data. My personal choice is to use tf.data pipelines. Please check this blog to know more about it.\nPlease try to check for the duplicate operations like creating multiple models, storing temporary variables in the GPU memory.\n\n\n\nWhat if we Underfit to the data\nsome suggestions to make in decreasing order of priority\n\nMake your model bigger\nReduce/Remove regularization(L1/L2/Dropout) if any.\nDo error analysis. based on this try to change the preprocessing/data if needed.\nRead technical papers and choose the state of the art models.\nTune hyperparameters\nAdd custom features if needed.\n\n\n\nWhat if we Overfit to the data\nsome suggestions to make in decreasing order of priority\n\nAdd more training data\nAdd normalization layers(BN, layer norm)\nAdd data augmentation\nIncrease regularization\nDo error analysis. based on this try to change the preprocessing/data if needed.\nChoose a different model\nTune hyperparameters\n\n You can check some of my other blogs at this link. This is my LinkedIn and GitHub"
  }
]